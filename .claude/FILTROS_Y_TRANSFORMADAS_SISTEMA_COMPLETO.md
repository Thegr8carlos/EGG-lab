# Sistema Completo de Filtros y Transformadas

**Fecha de Finalizaci√≥n**: 2025-11-02
**Estado**: ‚úÖ COMPLETAMENTE FUNCIONAL

---

## Resumen Ejecutivo

El sistema de filtros y transformadas de EGG-Lab est√° completamente implementado y funcional. Permite a los investigadores aplicar filtros de procesamiento de se√±ales y transformadas sobre datos EEG de manera interactiva, con visualizaci√≥n en tiempo real de resultados.

---

## P√°ginas Implementadas

### 1. P√°gina de Filtros (`/filtros`)

**Prop√≥sito**: Aplicar filtros de procesamiento de se√±ales EEG y comparar se√±al original vs filtrada.

**Filtros disponibles**:
- ‚úÖ **ICA** - Independent Component Analysis
- ‚úÖ **WaveletsBase** - Denoising por wavelets
- ‚úÖ **BandPass** - Filtros paso banda/alto/bajo (con auto-ajuste de filter_length)
- ‚úÖ **Notch** - Filtro notch para eliminar ruido de l√≠nea el√©ctrica (50/60 Hz)

**Caracter√≠sticas**:
- Vista de dos columnas (Original azul vs Filtrada morada oscurecida)
- Navegaci√≥n de canales (8 canales por p√°gina)
- Filtrado por clase de eventos (abajo, arriba, derecha, izquierda, Todas)
- Selecci√≥n de canales espec√≠ficos con checklist scrollable
- Botones de ayuda: "Todos", "Limpiar", "Solo EEG"
- Guardado autom√°tico en `Events/filtered/{evento}_{tipo}_{id}.npy`
- Registro en experimento actual

### 2. P√°gina de Transformadas (`/extractores`)

**Prop√≥sito**: Aplicar transformadas de extracci√≥n de caracter√≠sticas sobre se√±ales EEG.

**Transformadas disponibles**:
- ‚úÖ **WaveletTransform** - Transformada wavelet con 60+ opciones de wavelets
- ‚úÖ **FFTTransform** - Fast Fourier Transform con ventanas configurables
- ‚úÖ **DCTTransform** - Discrete Cosine Transform con normalizaci√≥n opcional
- ‚úÖ **WindowingTransform** - Ventaneo de se√±ales con diferentes configuraciones

**Caracter√≠sticas**:
- Vista de dos columnas (Original vs Transformada con color oscurecido)
- Mismo sistema de navegaci√≥n y filtrado que filtros
- Generaci√≥n autom√°tica de etiquetas para eventos individuales
- Manejo de arrays 3D (concatenaci√≥n de frames para visualizaci√≥n)
- Guardado en `Events/transformed/{evento}_{tipo}_{id}.npy`
- Guardado de etiquetas en `Events/transformed_labels/`
- Registro en experimento actual

---

## Arquitectura del Sistema

### Componente RightColumn

**Archivo**: `src/app/components/RigthComlumn.py`

**Funci√≥n principal**: Generaci√≥n din√°mica de formularios desde schemas Pydantic.

**Tipos de ventanas soportadas**:
- `"filter"` ‚Üí Filtros de se√±ales
- `"featureExtracture"` ‚Üí Transformadas/extractores
- `"clasificationModelsP300"` ‚Üí Modelos para paradigma P300
- `"clasificationModelsInner"` ‚Üí Modelos para paradigma Inner Speech

**Detecci√≥n autom√°tica de tipos de campos**:

1. **Enums/Literals** ‚Üí Dropdowns
   ```python
   method: Literal["fastica", "infomax", "picard"]
   # Genera: Dropdown con 3 opciones
   ```

2. **Union types** ‚Üí Inputs num√©ricos/texto con validaci√≥n
   ```python
   freq: Union[float, Tuple[float, float]]
   # Genera: Input text con placeholder "Ej: 30 o 1,30"
   ```

3. **Arrays** ‚Üí Input text separado por comas
   ```python
   freqs: List[float]
   # Genera: Input "8, 12, 30, 45"
   ```

4. **Optional[Literal[..., None]]** ‚Üí Dropdown con "None" como string
   ```python
   norm: Optional[Literal["ortho", None]]
   # Genera: Dropdown ["ortho", "None"]
   # Callback convierte "None" ‚Üí Python None
   ```

**Traducci√≥n autom√°tica a espa√±ol**: `NOMBRE_CAMPOS_ES` (30+ traducciones)

### FilterSchemaFactory

**Archivo**: `src/backend/classes/Filter/FilterSchemaFactory.py`

**Funciones principales**:
- `get_all_filter_schemas()` - Obtiene schemas JSON de todos los filtros
- `filterCallbackRegister(boton_id, inputs_map)` - Registra callbacks din√°micamente

**Flujo de aplicaci√≥n de filtros**:
```
Usuario llena formulario
    ‚Üì
Clic en "Aplicar" (btn-aplicar-{FilterName})
    ‚Üì
Callback valida con Pydantic
    ‚Üì
Genera ID autoincremental
    ‚Üì
Aplica: Filter.apply(instance, file_path, directory_path_out)
    ‚Üì
Guarda: Events/filtered/{evento}_{sufijo}_{id}.npy
    ‚Üì
Actualiza: filtered-signal-store-filtros
    ‚Üì
UI renderiza columna derecha autom√°ticamente
```

**Preprocesamiento de inputs**:
- Conversi√≥n de strings con comas a arrays: `"1,30"` ‚Üí `[1.0, 30.0]`
- Conversi√≥n de string "None" a Python `None`
- Auto-poblaci√≥n de frecuencia de muestreo (`sp`) desde signal_data

### TransformSchemaFactory

**Archivo**: `src/backend/classes/FeatureExtracture/TransformSchemaFactory.py`

**Funciones principales**:
- `get_all_transform_schemas()` - Obtiene schemas JSON de todas las transformadas
- `TransformCallbackRegister(boton_id, inputs_map)` - Registra callbacks din√°micamente

**Flujo de aplicaci√≥n de transformadas**:
```
Usuario llena formulario
    ‚Üì
Clic en "Aplicar" (btn-aplicar-{TransformName})
    ‚Üì
Callback valida con Pydantic
    ‚Üì
Obtiene path del evento desde signal-store-extractores
    ‚Üì
Genera etiquetas temporales (extrae clase del nombre del archivo)
    ‚Üì
Aplica: Transform.apply(instance, file_path_in, directory_path_out, labels_directory, labels_out_path)
    ‚Üì
Guarda: Events/transformed/{evento}_{sufijo}_{id}.npy
         Events/transformed_labels/{evento}_{sufijo}_{id}_labels.npy
    ‚Üì
Carga datos transformados (manejo de arrays 3D ‚Üí 2D)
    ‚Üì
Actualiza: transformed-signal-store-extractores
    ‚Üì
UI renderiza columna derecha autom√°ticamente
```

**Caracter√≠sticas especiales**:
- Generaci√≥n autom√°tica de etiquetas temporales para eventos individuales
- Manejo de arrays 3D: `(n_frames, frame_size, n_channels)` ‚Üí `(n_channels, n_frames * frame_size)`
- Limpieza de archivos temporales despu√©s de aplicar
- Sistema de colores din√°micos con `get_class_color()`

---

<<<<<<< HEAD
=======
## Sistema de Pipeline de Historial (Pipeline History System)

**Estado**: ‚úÖ Fase 1 Completada (Backend Core) - 2025-11-03

### Prop√≥sito

El Sistema de Pipeline de Historial permite aplicar **TODOS** los filtros y transformadas del experimento de forma secuencial y autom√°tica, con caching inteligente para optimizar el rendimiento. En lugar de aplicar filtros y transformadas uno por uno, el sistema:

1. Aplica todos los filtros en orden
2. Luego aplica todas las transformadas en orden
3. Guarda el resultado final en cach√©
4. En navegaciones subsecuentes, carga directamente desde cach√© (hasta 100x m√°s r√°pido)

### Arquitectura de Backend (Fase 1)

**Archivo**: `src/backend/classes/Experiment.py`

Se agregaron 6 nuevos m√©todos para gestionar el pipeline completo:

#### 1. `apply_history_pipeline()`

**L√≠neas**: 425-772

**Prop√≥sito**: M√©todo principal que ejecuta el pipeline completo con caching inteligente.

**Par√°metros**:
- `file_path`: Path al archivo .npy del evento
- `force_recalculate`: Si es True, ignora cach√© y recalcula (default: False)
- `save_intermediates`: Si es True, guarda resultados intermedios (default: True)
- `verbose`: Si es True, imprime mensajes de progreso (default: True)

**Retorna**:
```python
{
    "signal": np.ndarray,           # Se√±al transformada final
    "metadata": dict,                # Info de ejecuci√≥n
    "cache_used": bool,              # True si us√≥ cach√©
    "cache_path": str                # Path al archivo de cach√©
}
```

**Workflow**:
```
1. Cargar experimento actual
2. Construir paths de cach√© usando _get_pipeline_cache_path()
3. Verificar si existe cach√© v√°lido:
   - Existe cache_file y metadata_file?
   - Hash del pipeline coincide?
   - Si S√ç ‚Üí Cargar desde cach√© y retornar inmediatamente ‚ö°
4. Si NO existe cach√© v√°lido:
   - Fase 1: Aplicar todos los filtros secuencialmente
   - Fase 2: Aplicar todas las transformadas secuencialmente
   - Guardar resultado final en cache_file
   - Guardar metadata con hash del pipeline
5. Retornar resultado con metadata
```

**Caching inteligente**:
```python
# Hash del pipeline = MD5(JSON serializado de filtros + transforms)
current_config = {
    "filters": experiment.filters,
    "transforms": experiment.transform
}
pipeline_hash = hashlib.md5(
    json.dumps(current_config, sort_keys=True).encode()
).hexdigest()

# Si el hash coincide ‚Üí La configuraci√≥n no ha cambiado ‚Üí Cach√© v√°lido
if cached_metadata.get("pipeline_hash") == current_hash:
    return cached_signal  # ‚ö° Instant load
```

**Beneficios del cach√©**:
- ‚úÖ Primera carga: ~15-30 segundos (depende del n√∫mero de filtros/transforms)
- ‚úÖ Cargas subsecuentes: ~0.15 segundos (hasta 100x m√°s r√°pido)
- ‚úÖ Cach√© invalidado autom√°ticamente si cambia la configuraci√≥n del experimento
- ‚úÖ Cada evento tiene su propio cach√© (basado en MD5 del nombre del archivo)

#### 2. `_get_pipeline_cache_path()`

**L√≠neas**: 354-382

**Prop√≥sito**: Construye paths de archivos de cach√© para un evento espec√≠fico.

**Estructura de carpetas creada**:
```
Dataset/
‚îî‚îÄ‚îÄ Events/
    ‚îî‚îÄ‚îÄ Aux/
        ‚îî‚îÄ‚îÄ experiment_{id}/
            ‚îú‚îÄ‚îÄ pipeline_cache/              # Resultados finales cacheados
            ‚îÇ   ‚îú‚îÄ‚îÄ {evento}_{hash}_final.npy
            ‚îÇ   ‚îî‚îÄ‚îÄ {evento}_{hash}_metadata.json
            ‚îî‚îÄ‚îÄ intermediates/               # Pasos intermedios (debug)
                ‚îú‚îÄ‚îÄ step_00_ICA_0.npy
                ‚îú‚îÄ‚îÄ step_01_WaveletsBase_1.npy
                ‚îú‚îÄ‚îÄ step_02_WaveletTransform_0.npy
                ‚îî‚îÄ‚îÄ step_03_FFTTransform_1.npy
```

**Hash del archivo**: Se usa MD5 de los primeros 8 caracteres del nombre del archivo para evitar colisiones y manejar nombres con caracteres especiales `[`, `]`, `{`, `}`.

#### 3. `_reconstruct_filter_instance()`

**L√≠neas**: 384-402

**Prop√≥sito**: Reconstruye una instancia Pydantic de filtro desde la configuraci√≥n JSON del experimento.

**Workflow**:
```python
# JSON en experiment.filters:
{
    "id": 0,
    "ICA": {
        "id": "0",
        "sp": 1024.0,
        "numeroComponentes": 13,
        "method": "fastica"
    }
}

# ‚Üì _reconstruct_filter_instance("ICA", {...})

# Instancia Pydantic:
ICA(id="0", sp=1024.0, numeroComponentes=13, method="fastica")
```

**Uso en pipeline**:
```python
filter_instance = cls._reconstruct_filter_instance("ICA", config)
filter_class.apply(filter_instance, file_path_in, file_path_out)
```

#### 4. `_reconstruct_transform_instance()`

**L√≠neas**: 404-422

**Prop√≥sito**: Reconstruye una instancia Pydantic de transformada desde la configuraci√≥n JSON del experimento.

**Workflow**: Id√©ntico a `_reconstruct_filter_instance()` pero para transformadas.

**Uso en pipeline**:
```python
transform_instance = cls._reconstruct_transform_instance("WaveletTransform", config)
transform_class.apply(transform_instance, file_path_in, directory_path_out, ...)
```

#### 5. `get_experiment_summary()`

**L√≠neas**: 774-840

**Prop√≥sito**: Retorna un resumen del experimento actual para mostrar en la UI.

**Retorna**:
```python
{
    "experiment_id": "573",
    "filters": [
        {"id": 0, "name": "ICA", "config": {...}},
        {"id": 1, "name": "WaveletsBase", "config": {...}}
    ],
    "transforms": [
        {"id": 0, "name": "WaveletTransform", "config": {...}},
        {"id": 1, "name": "FFTTransform", "config": {...}}
    ],
    "total_steps": 4,
    "cache_info": {
        "size_mb": 45.23,
        "files_count": 120
    }
}
```

**Uso futuro**: Este m√©todo ser√° usado por el componente UI del visor de historial (Fase 2) para mostrar:
- Accordion con todos los pasos del pipeline
- Tama√±o del cach√©
- Bot√≥n "Ver JSON" para cada filtro/transformada

#### 6. `clear_pipeline_cache()`

**L√≠neas**: 842-899

**Prop√≥sito**: Elimina todos los archivos de cach√© del pipeline para un experimento espec√≠fico.

**Uso**:
```python
# Limpiar cach√© del experimento actual
result = Experiment.clear_pipeline_cache()

# Limpiar cach√© de un experimento espec√≠fico
result = Experiment.clear_pipeline_cache(experiment_id="573")
```

**Retorna**:
```python
{
    "files_deleted": 240,
    "space_freed_mb": 156.78,
    "experiments_affected": ["573"]
}
```

**Qu√© elimina**:
- Todos los archivos en `pipeline_cache/` (archivos .npy y .json)
- Todos los archivos en `intermediates/` (pasos intermedios)
- Las carpetas se eliminan completamente

---

### Flujo de Ejecuci√≥n del Pipeline

**Ejemplo con 2 filtros + 2 transformadas**:

```
apply_history_pipeline("/dataset/Events/abajo[123]{456}.npy")
    ‚Üì
1. ¬øExiste cach√© v√°lido?
   - Buscar: Aux/experiment_573/pipeline_cache/abajo[123]{456}_a1b2c3d4_final.npy
   - Verificar hash del pipeline
   - S√ç existe y es v√°lido ‚Üí RETORNAR INMEDIATAMENTE ‚ö°
    ‚Üì
2. NO existe cach√© ‚Üí Ejecutar pipeline completo:

   FASE 1: FILTROS
   ================
   Se√±al original: (137 channels, 2612 samples)
       ‚Üì
   ‚Üí Aplicar ICA (id=0)
       signal ‚Üí temp_input.npy
       ICA.apply() ‚Üí temp_output.npy
       Guardar: intermediates/step_00_ICA_0.npy
       current_signal ‚Üê temp_output.npy
   Se√±al despu√©s de ICA: (137 channels, 2612 samples)
       ‚Üì
   ‚Üí Aplicar WaveletsBase (id=1)
       signal ‚Üí temp_input.npy
       WaveletsBase.apply() ‚Üí temp_output.npy
       Guardar: intermediates/step_01_WaveletsBase_1.npy
       current_signal ‚Üê temp_output.npy
   Se√±al despu√©s de Wavelets: (137 channels, 2612 samples)
       ‚Üì

   FASE 2: TRANSFORMADAS
   =====================
   ‚Üí Aplicar WaveletTransform (id=0)
       signal ‚Üí temp_input.npy
       Generar etiquetas temporales
       WaveletTransform.apply() ‚Üí temp_output_dir/*.npy
       Manejar array 3D si es necesario
       Guardar: intermediates/step_02_WaveletTransform_0.npy
       current_signal ‚Üê procesado
   Se√±al despu√©s de WaveletTransform: (87, 30, 137) ‚Üí (137, 2610)
       ‚Üì
   ‚Üí Aplicar FFTTransform (id=1)
       signal ‚Üí temp_input.npy
       Generar etiquetas temporales
       FFTTransform.apply() ‚Üí temp_output_dir/*.npy
       Guardar: intermediates/step_03_FFTTransform_1.npy
       current_signal ‚Üê procesado
   Se√±al despu√©s de FFT: (137, 1305)
       ‚Üì

3. Guardar resultado final:
   - Guardar: pipeline_cache/abajo[123]{456}_a1b2c3d4_final.npy
   - Metadata: {
       "pipeline_hash": "a1b2c3d4...",
       "execution_time_seconds": 18.34,
       "original_shape": [137, 2612],
       "final_shape": [137, 1305],
       "steps_applied": 4,
       "execution_log": [...]
     }
   - Guardar: pipeline_cache/abajo[123]{456}_a1b2c3d4_metadata.json
    ‚Üì
4. Retornar resultado con metadata

======================
Pr√≥xima carga del mismo evento:
======================
apply_history_pipeline("/dataset/Events/abajo[123]{456}.npy")
    ‚Üì
1. ¬øExiste cach√© v√°lido?
   - S√ç existe
   - Hash coincide
   - Cargar: pipeline_cache/abajo[123]{456}_a1b2c3d4_final.npy
   - RETORNAR INMEDIATAMENTE (0.15s vs 18.34s) ‚ö°‚ö°‚ö°
```

---

### Metadata del Pipeline

Cada archivo de cach√© tiene un archivo `_metadata.json` asociado con informaci√≥n detallada:

```json
{
  "pipeline_hash": "a1b2c3d4e5f6g7h8",
  "experiment_id": "573",
  "original_file": "/dataset/Events/abajo[123]{456}.npy",
  "original_shape": [137, 2612],
  "final_shape": [137, 1305],
  "execution_time_seconds": 18.34,
  "steps_applied": 4,
  "execution_log": [
    {
      "step": 0,
      "type": "filter",
      "name": "ICA",
      "id": 0,
      "shape": [137, 2612]
    },
    {
      "step": 1,
      "type": "filter",
      "name": "WaveletsBase",
      "id": 1,
      "shape": [137, 2612]
    },
    {
      "step": 2,
      "type": "transform",
      "name": "WaveletTransform",
      "id": 0,
      "shape": [137, 2610]
    },
    {
      "step": 3,
      "type": "transform",
      "name": "FFTTransform",
      "id": 1,
      "shape": [137, 1305]
    }
  ],
  "timestamp": 1730678123.456
}
```

**Usos de metadata**:
- ‚úÖ Validaci√≥n de cach√© (comparar `pipeline_hash`)
- ‚úÖ Debugging (ver shapes en cada paso)
- ‚úÖ An√°lisis de performance (execution_time_seconds)
- ‚úÖ UI del visor de historial (mostrar execution_log)
- ‚úÖ Tracking de cambios dimensionales

---

### Manejo de Errores y Robustez

El pipeline est√° dise√±ado para ser robusto ante fallos:

**1. Filtro/Transformada falla**:
```python
try:
    success = filter_class.apply(...)
    if success:
        current_signal = load_output()
    else:
        print("‚ö†Ô∏è Filtro fall√≥, continuando con se√±al anterior")
        # Contin√∫a con la se√±al anterior, no aborta el pipeline
except Exception as e:
    print(f"‚ùå Error: {e}")
    continue  # Salta al siguiente paso
```

**2. Configuraci√≥n inv√°lida**:
```python
if not filter_name or not filter_config:
    print(f"‚ö†Ô∏è Filtro {filter_id} sin configuraci√≥n v√°lida, saltando")
    continue
```

**3. Cleanup de archivos temporales**:
```python
# Siempre se limpian, incluso si hay error
if temp_input.exists():
    temp_input.unlink()
if temp_output.exists():
    temp_output.unlink()
```

**Resultado**: El pipeline completa tantos pasos como sea posible, incluso si algunos fallan.

---

### Pr√≥ximas Fases

**Fase 2: UI Viewer** (Pendiente)
- Componente accordion para visualizar historial
- Botones "Ver JSON" para cada filtro/transformada
- Bot√≥n "Limpiar Cach√©"
- Mostrar tama√±o del cach√© y cantidad de archivos

**Fase 3: Auto Integration** (Pendiente)
- Toggle global para activar/desactivar pipeline autom√°tico (ON por defecto)
- Integraci√≥n con callbacks de navegaci√≥n en `/filtros` y `/extractores`
- Al navegar entre eventos, aplicar pipeline autom√°ticamente
- Al aplicar nuevo filtro/transformada, invalidar cach√© autom√°ticamente

---

>>>>>>> eb8759879e9e26769687421c789cd6f7012457b4
## Sistema de Colores Din√°micos

**Archivo**: `src/shared/class_colors.py`

**Colores predefinidos por clase** (formato HSL):
```python
CLASS_COLORS = {
    "abajo": "hsl(0, 75%, 55%)",      # Rojo vibrante
    "arriba": "hsl(120, 70%, 50%)",   # Verde brillante
    "derecha": "hsl(210, 75%, 55%)",  # Azul cielo
    "izquierda": "hsl(45, 85%, 55%)", # Amarillo/dorado
    "target": "hsl(270, 70%, 55%)",
    "non-target": "hsl(180, 65%, 50%)",
}
```

**Funci√≥n principal**:
```python
get_class_color(class_name: str, index: int = 0) -> str
```
- Retorna color HSL para una clase
- Fallback: Genera color consistente basado en hash del nombre
- Normalizaci√≥n autom√°tica: espacios ‚Üí guiones bajos, lowercase

**Integraci√≥n en plots**:
- **Columna izquierda (Original)**: Color brillante de la clase
- **Columna derecha (Procesada)**: Color oscurecido con `darkenHSL(classColor, 20)`
- **T√≠tulos**: Borde superior con color de clase

**Funci√≥n JavaScript `darkenHSL()`**:
```javascript
function darkenHSL(hslColor, amount = 20) {
  const match = hslColor.match(/hsl\((\d+),\s*(\d+)%,\s*(\d+)%\)/);
  const h = parseInt(match[1]);
  const s = parseInt(match[2]);
  const l = parseInt(match[3]);
  const newL = Math.max(0, l - amount);
  return `hsl(${h}, ${s}%, ${newL}%)`;
}
```

---

## Flujo de Datos Completo

### Filtros (filtros.py)

```
1. Usuario selecciona dataset ‚Üí Metadata cargada
    ‚Üì
2. Usuario selecciona archivo ‚Üí Dataset.get_events_by_class()
    ‚Üì
3. Usuario aplica filtro ‚Üí filterCallbackRegister()
    ‚Üì
4. Validaci√≥n Pydantic ‚Üí Filter.apply()
    ‚Üì
5. Guardado en Events/filtered/
    ‚Üì
6. Actualizaci√≥n de filtered-signal-store-filtros
    ‚Üì
7. Clientside callback renderiza columna derecha
    ‚Üì
8. Visualizaci√≥n con color oscurecido
```

### Transformadas (extractores.py)

```
1. Usuario selecciona dataset ‚Üí Metadata cargada
    ‚Üì
2. Usuario selecciona archivo ‚Üí Dataset.get_events_by_class()
    ‚Üì
3. Usuario aplica transformada ‚Üí TransformCallbackRegister()
    ‚Üì
4. Generaci√≥n de etiquetas temporales
    ‚Üì
5. Validaci√≥n Pydantic ‚Üí Transform.apply()
    ‚Üì
6. Guardado en Events/transformed/ y Events/transformed_labels/
    ‚Üì
7. Manejo de arrays 3D ‚Üí 2D (si aplica)
    ‚Üì
8. Actualizaci√≥n de transformed-signal-store-extractores
    ‚Üì
9. Clientside callback renderiza columna derecha
    ‚Üì
10. Visualizaci√≥n con color oscurecido
```

---

## Stores de Dash

### Filtros

```python
EVENTS_STORE_ID = "events-store-filtros"
DATA_STORE_ID = "signal-store-filtros"
FILTERED_DATA_STORE_ID = "filtered-signal-store-filtros"
CHANNEL_RANGE_STORE = "channel-range-store-filtros"
SELECTED_CLASS_STORE = "selected-class-store-filtros"
SELECTED_CHANNELS_STORE = "selected-channels-store-filtros"
```

### Transformadas

```python
EVENTS_STORE_ID = "events-store-extractores"
DATA_STORE_ID = "signal-store-extractores"
TRANSFORMED_DATA_STORE_ID = "transformed-signal-store-extractores"
CHANNEL_RANGE_STORE = "channel-range-store-extractores"
SELECTED_CLASS_STORE = "selected-class-store-extractores"
SELECTED_CHANNELS_STORE = "selected-channels-store-extractores"
```

---

## Caracter√≠sticas Avanzadas

### 1. Navegaci√≥n de Canales

**Modos de visualizaci√≥n**:

1. **Paginaci√≥n** (sin selecci√≥n espec√≠fica):
   - 8 canales por p√°gina
   - Botones "‚Üê Anteriores" / "Siguientes ‚Üí"
   - Texto informativo: "Canales 0 - 7 de 137"

2. **Canales espec√≠ficos** (con selecci√≥n):
   - Checklist scrollable con todos los canales
   - Botones de ayuda: "Todos", "Limpiar", "Solo EEG"
   - Contador: "128 canales seleccionados"
   - Deshabilita navegaci√≥n por p√°ginas

**Callbacks de navegaci√≥n**:
- `populate_channel_checklist()` - Llena checklist con nombres de canales
- `save_selected_channels()` - Guarda selecci√≥n en store
- `update_channel_count()` - Actualiza contador
- `handle_channel_buttons()` - Maneja botones de ayuda

### 2. Filtrado por Clase

**Funcionamiento**:
- Botones por clase: "abajo", "arriba", "derecha", "izquierda"
- Bot√≥n "Todas" para mostrar eventos sin filtrar
- Selecci√≥n √∫nica (un bot√≥n activo a la vez)
- Backend: `Dataset.get_events_by_class(path, class_name)`
- Callback: `select_specific_class()` y `select_all_classes()`

**Flujo**:
```
Usuario selecciona archivo ‚Üí Determina sesi√≥n
    ‚Üì
Usuario hace clic en "derecha"
    ‚Üì
SELECTED_CLASS_STORE = "derecha"
    ‚Üì
Callback se dispara con selected_class="derecha"
    ‚Üì
Dataset.get_events_by_class(path, "derecha")
    ‚Üì
Retorna primer evento de clase "derecha" en esa sesi√≥n
    ‚Üì
Carga y muestra ese evento
```

### 3. Generaci√≥n Autom√°tica de Etiquetas

**Problema**: Las transformadas requieren archivos de etiquetas, pero los eventos individuales en `Events/` no tienen etiquetas separadas (la clase est√° en el nombre del archivo).

**Soluci√≥n** (`TransformSchemaFactory.py:232-251`):

```python
# Extraer clase del nombre del archivo
# "abajo[439.357]{441.908}.npy" ‚Üí "abajo"
file_name = p_in.stem
event_class = file_name.split('[')[0].strip()

# Crear directorio temporal para etiquetas
labels_dir = p_in.parent / "temp_labels"
labels_dir.mkdir(parents=True, exist_ok=True)

# Generar array de etiquetas (todas con la misma clase)
arr_signal = np.load(str(p_in), allow_pickle=False)
n_samples = arr_signal.shape[1] if arr_signal.ndim == 2 else arr_signal.shape[0]
labels_array = np.array([event_class] * n_samples, dtype=str)

# Guardar etiquetas temporales
temp_labels_file = labels_dir / p_in.name
np.save(str(temp_labels_file), labels_array)
```

**Limpieza autom√°tica** (l√≠neas 266-270):
```python
# Limpiar archivo temporal despu√©s de aplicar
if temp_labels_file.exists():
    temp_labels_file.unlink()
```

### 4. Manejo de Arrays 3D

**Problema**: Las transformadas ventaneadas generan arrays 3D `(n_frames, frame_size, n_channels)`, pero la visualizaci√≥n espera 2D `(n_channels, n_times)`.

**Soluci√≥n** (`TransformSchemaFactory.py:299-310`):

```python
if arr.ndim == 3:
    # Formato: (n_frames, frame_size, n_channels)
    # Objetivo: (n_channels, n_frames * frame_size)
    n_frames, frame_size, n_channels = arr.shape

    # Paso 1: Transponer ‚Üí (n_channels, n_frames, frame_size)
    arr_transposed = arr.transpose(2, 0, 1)

    # Paso 2: Concatenar frames ‚Üí (n_channels, n_frames * frame_size)
    arr = arr_transposed.reshape(n_channels, n_frames * frame_size)

    print(f"Array 3D concatenado: {arr.shape} (canales x tiempo)")
```

---

## Mejoras Implementadas

### 1. Validaciones Context-Aware y Feedback Visual (2025-11-02) ‚ú®

**Problema reportado**:
- Dropdown de wavelets mostraba 60+ opciones para filtros, pero `WaveletsBase` solo acepta 16 espec√≠ficos
- Errores de validaci√≥n solo aparec√≠an en consola, usuario no ten√≠a feedback visual
- Usuario seleccionaba opciones inv√°lidas sin saber por qu√© fallaba

**Soluci√≥n implementada**:

#### A. Dropdown Diferenciado para Wavelets

**Detecci√≥n autom√°tica del contexto** (`RigthComlumn.py:82-90`):
```python
# Detectar si es WaveletsBase (filtro) o WaveletTransform (transformada)
is_filter = "WaveletsBase" in type

if is_filter:
    # WaveletsBase: Solo 16 wavelets v√°lidos seg√∫n el Literal del modelo
    valid_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db8',
                    'sym2', 'sym3', 'sym4', 'sym5',
                    'coif1', 'coif2', 'coif3', 'coif5', 'haar']
    dropdown_options = [{"label": w, "value": w} for w in valid_wavelets]
else:
    # WaveletTransform: Cat√°logo completo de 60+ wavelets
    wavelet_families = {
        "Daubechies": [f"db{i}" for i in range(1, 39)],
        "Symlets": [f"sym{i}" for i in range(2, 21)],
        # ... m√°s familias
    }
```

**Beneficios**:
- ‚úÖ **Filtros** (`WaveletsBase`): Solo muestra las 16 opciones v√°lidas
- ‚úÖ **Transformadas** (`WaveletTransform`): Muestra cat√°logo completo (60+)
- ‚úÖ Usuario no puede seleccionar opciones inv√°lidas
- ‚úÖ Previene errores de validaci√≥n antes de enviar

**Wavelets v√°lidos para filtros**:
| Familia | Wavelets |
|---------|----------|
| Daubechies | db1, db2, db3, db4, db5, db6, db8 |
| Symlets | sym2, sym3, sym4, sym5 |
| Coiflets | coif1, coif2, coif3, coif5 |
| Haar | haar |

#### B. Feedback Visual de Errores en Botones

**Mensajes de error visibles** (implementado en ambas Factories):

**1. FilterSchemaFactory.py (l√≠neas 224-242)**:
```python
except ValidationError as e:
    errores = e.errors()
    # Construir mensaje de error legible
    error_fields = [err['loc'][0] for err in errores if err['loc']]
    msg_short = f"‚ùå Error: {', '.join(error_fields)}"
    # Retornar mensaje en el bot√≥n
    return msg_short, no_update

except ValueError as e:
    error_msg = f"‚ùå Error: {str(e)}"
    return error_msg, no_update

except Exception as e:
    return f"‚ùå Error inesperado", no_update
```

**2. TransformSchemaFactory.py (l√≠neas 390-403)** - Misma implementaci√≥n

**Tipos de mensajes de error**:

| Tipo de Error | Mensaje en Bot√≥n | Cu√°ndo Ocurre |
|---------------|------------------|---------------|
| ValidationError | `‚ùå Error: wavelet, threshold` | Campos con valores inv√°lidos |
| ValueError | `‚ùå Error: Nivel inv√°lido: 6. Permitido hasta 5` | Validaci√≥n de backend |
| Sin se√±al cargada | `‚ùå No hay se√±al cargada` | Usuario no ha cargado evento |
| Archivo no encontrado | `‚ùå Archivo no encontrado` | Error en procesamiento |
| Error inesperado | `‚ùå Error inesperado` | Excepciones no manejadas |

**Ejemplo de flujo con error**:
```
Usuario selecciona wavelet="rbio3.1" en filtro
    ‚Üì
Hace clic en "Aplicar"
    ‚Üì
Backend valida con Pydantic
    ‚Üì
ValidationError: wavelet debe ser uno de [db1, db2, ..., haar]
    ‚Üì
Bot√≥n muestra: "‚ùå Error: wavelet"
    ‚Üì
Usuario ve error inmediatamente sin revisar consola
```

**Ejemplo de flujo exitoso**:
```
Usuario selecciona wavelet="db4" en filtro
    ‚Üì
Hace clic en "Aplicar"
    ‚Üì
Backend valida con Pydantic ‚úÖ
    ‚Üì
Filtro se aplica correctamente
    ‚Üì
Bot√≥n mantiene texto: "Aplicar"
    ‚Üì
Columna derecha muestra se√±al filtrada
```

**Beneficios**:
- ‚úÖ Usuario ve errores inmediatamente en la UI
- ‚úÖ Mensajes concisos y accionables
- ‚úÖ No necesita revisar consola
- ‚úÖ Experiencia de usuario mejorada significativamente
- ‚úÖ Mensajes en espa√±ol (formato consistente con la app)

**Archivos modificados**:
- `RigthComlumn.py` (l√≠neas 76-122): Dropdown context-aware
- `FilterSchemaFactory.py` (l√≠neas 140-142, 189-201, 223-242): Feedback visual
- `TransformSchemaFactory.py` (l√≠neas 164-166, 177-179, 283-285, 321-324, 390-403): Feedback visual

### 2. Auto-ajuste de filter_length en BandPass (2025-10-28)

**Problema**: FIR filters fallan con `filter_length` muy corto.

**Soluci√≥n** (`BandPass.py:104-146`):

```python
if instance.method == "fir":
    if instance.order is not None:
        filter_length = instance.order
        if filter_length % 2 == 0:
            filter_length += 1  # MNE requiere impar
    else:
        filter_length = "auto"

    try:
        out = mne.filter.filter_data(..., filter_length=filter_length)
    except ValueError as e:
        if "too short" in str(e) and filter_length != "auto":
            # Fallback autom√°tico
            out = mne.filter.filter_data(..., filter_length="auto")
        else:
            raise
```

### 3. Fix para Dropdowns con valores `None` (2025-11-02)

**Problema**: Campos tipo `Optional[Literal["ortho", None]]` generaban error en Dash Dropdown.

**Causa**: Dash Dropdown no acepta `null` como valor v√°lido.

**Soluci√≥n**:

1. **RightColumn** (l√≠neas 99-111):
   ```python
   dropdown_options = []
   has_none = False
   for val in enum_values:
       if val is None:
           has_none = True
       else:
           dropdown_options.append({"label": str(val), "value": val})

   if has_none:
       dropdown_options.append({"label": "None", "value": "None"})
   ```

2. **Callbacks** (FilterSchemaFactory y TransformSchemaFactory):
   ```python
   if isinstance(value, str) and value == "None":
       datos[field] = None
   ```

### 4. Preprocesamiento de Union Types

**Problema**: Campos como `freq` en BandPass pueden ser `float` O `Tuple[float, float]`.

**Soluci√≥n** (ambas Factory):

```python
if isinstance(value, str) and "," in value:
    try:
        valores_separados = [float(v.strip()) for v in value.split(",")]
        datos[field] = valores_separados
    except (ValueError, AttributeError):
        datos[field] = value
```

**UI**:
```python
if has_number and has_array:
    inputType = "text"
    placeholder = "Ej: 30 (un valor) o 1,30 (dos valores separados por coma)"
```

### 5. Frecuencia de Muestreo Autom√°tica

**Problema**: El campo `sp` es requerido pero el usuario no deber√≠a ingresarlo manualmente.

**Soluci√≥n** (ambas Factory):

```python
# Obtener sp del signal_data si no viene del formulario
if "sp" not in datos or datos.get("sp") is None:
    sfreq = signal_data.get("sfreq", 1024.0)
    datos["sp"] = float(sfreq)
    print(f"üìä Usando frecuencia de muestreo: {sfreq} Hz")
```

---

## Convenciones de C√≥digo

### Nomenclatura de IDs

**Formularios**:
```python
id=f"{type}-{field_name}"
# Ejemplos:
# - "ICA-sp"
# - "BandPass-freq"
# - "WaveletTransform-wavelet"
```

**Botones**:
```python
id=f"btn-aplicar-{type}"
# Ejemplos:
# - "btn-aplicar-ICA"
# - "btn-aplicar-WaveletTransform"
```

### Nomenclatura de Archivos Procesados

**Filtros**:
```
{evento}_{sufijo}_{id}.npy

Ejemplos:
- abajo[439.357]{441.908}_ica_0.npy
- abajo[439.357]{441.908}_bandpass_1.npy
- abajo[439.357]{441.908}_wav_2.npy
- abajo[439.357]{441.908}_notch_3.npy
```

**Transformadas**:
```
{evento}_{sufijo}_{id}.npy
{evento}_{sufijo}_{id}_labels.npy

Ejemplos:
- abajo[439.357]{441.908}_wavelet_0.npy
- abajo[439.357]{441.908}_wavelet_0_labels.npy
- abajo[439.357]{441.908}_fft_1.npy
- abajo[439.357]{441.908}_fft_1_labels.npy
```

### Mapeo de Sufijos

**Filtros**:
```python
filter_suffixes = {
    'ICA': 'ica',
    'WaveletsBase': 'wav',
    'BandPass': 'bandpass',
    'Notch': 'notch'
}
```

**Transformadas**:
```python
transform_suffixes = {
    "WaveletTransform": "wavelet",
    "FFTTransform": "fft",
    "DCTTransform": "dct",
    "WindowingTransform": "window"
}
```

---

## Testing y Validaci√≥n

### Tests Completados ‚úÖ

**Filtros**:
- ‚úÖ Aplicar ICA con canales espec√≠ficos
- ‚úÖ Aplicar Wavelets con diferentes wavelets
- ‚úÖ Aplicar BandPass con auto-ajuste de filter_length
- ‚úÖ Aplicar Notch en 50 Hz y 60 Hz
- ‚úÖ Filtrado por clase funciona correctamente
- ‚úÖ Navegaci√≥n de canales (paginaci√≥n y selecci√≥n espec√≠fica)
- ‚úÖ Visualizaci√≥n en columna derecha con color oscurecido

**Transformadas**:
- ‚úÖ Aplicar WaveletTransform con dropdowns de wavelets
- ‚úÖ Aplicar FFTTransform con ventanas configurables
- ‚úÖ Aplicar DCTTransform con normalizaci√≥n "ortho" y "None"
- ‚úÖ Generaci√≥n autom√°tica de etiquetas temporales
- ‚úÖ Manejo de arrays 3D ‚Üí 2D
- ‚úÖ Limpieza de archivos temporales
- ‚úÖ Visualizaci√≥n en columna derecha con color oscurecido

**RightColumn**:
- ‚úÖ Dropdowns con valores `None` funcionan correctamente
- ‚úÖ Conversi√≥n autom√°tica "None" (string) ‚Üí None (Python)
- ‚úÖ Preprocesamiento de arrays desde strings con comas
- ‚úÖ Traducci√≥n a espa√±ol de campos t√©cnicos
- ‚úÖ Validaci√≥n con Pydantic de todos los campos
- ‚úÖ Dropdown context-aware para wavelets (16 opciones en filtros, 60+ en transformadas)
- ‚úÖ Feedback visual de errores en botones
- ‚úÖ Mensajes de error legibles y accionables

---

## Documentaci√≥n Actualizada

Los siguientes archivos de documentaci√≥n han sido actualizados:

1. **`.claude/FILTROS_Y_TRANSFORMADAS_SISTEMA_COMPLETO.md`** (este archivo - √öltima actualizaci√≥n: 2025-11-02)
   - Nueva secci√≥n: "Validaciones Context-Aware y Feedback Visual"
   - Documentaci√≥n completa del dropdown diferenciado de wavelets
   - Sistema de feedback visual de errores en botones
   - Tabla de tipos de errores y mensajes
   - Ejemplos de flujos con y sin errores
   - Tests actualizados con nuevas caracter√≠sticas

2. **`.claude/components/RightColumn.md`** (√öltima actualizaci√≥n: 2025-11-02)
   - Nueva secci√≥n: "Dropdown Context-Aware para Wavelets"
   - Secci√≥n "Cambios Recientes" con fix de Dropdowns `None`
   - Flujo completo de conversi√≥n `None` ‚Üí "None" ‚Üí `None`
   - Documentaci√≥n de detecci√≥n autom√°tica filtro vs transformada

3. **`.claude/context.md`**
   - Nueva secci√≥n "Sistema Completo de Filtros y Transformadas"
   - Lista de caracter√≠sticas funcionando
   - Estado actual: ‚úÖ COMPLETAMENTE FUNCIONAL

---

## Pr√≥ximos Pasos Sugeridos

### Integraci√≥n con Modelos de Clasificaci√≥n

Ahora que filtros y transformadas est√°n completos, el siguiente paso natural es integrarlos con los modelos de clasificaci√≥n:

1. **Pipeline completo**: Dataset ‚Üí Filtros ‚Üí Transformadas ‚Üí Modelo ‚Üí Evaluaci√≥n
2. **Configuraci√≥n de experimentos**: Guardar configuraciones completas de preprocesamiento
3. **Comparaci√≥n de configuraciones**: Comparar diferentes combinaciones de filtros/transformadas
4. **Optimizaci√≥n de hiperpar√°metros**: B√∫squeda autom√°tica de mejores configuraciones

### Mejoras de UX

1. **Tooltips con descripciones**: Mostrar ayuda contextual en campos del formulario
2. **Validaci√≥n en frontend**: Validar rangos y tipos antes de enviar al backend
3. **Componente especial para arrays**: Editor visual para listas de valores
4. **Guardar/cargar configuraciones**: Presets de filtros/transformadas

### Performance

1. **Caching de resultados**: Evitar recomputar transformadas iguales
2. **Procesamiento en batch**: Aplicar filtros/transformadas a m√∫ltiples eventos
3. **Optimizaci√≥n de visualizaci√≥n**: Mejoras en renderizado de plots

---

## Conclusi√≥n

El sistema de filtros y transformadas de EGG-Lab est√° **completamente funcional** y listo para ser usado en investigaci√≥n. Todas las caracter√≠sticas clave han sido implementadas, probadas y documentadas. El sistema es robusto, extensible y mantiene una arquitectura limpia basada en generaci√≥n din√°mica desde schemas Pydantic.

**Estado final**: ‚úÖ PRODUCCI√ìN
