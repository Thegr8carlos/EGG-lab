"""
Configuraci√≥n GPU Global para TensorFlow

DEBE llamarse al inicio de la aplicaci√≥n, ANTES de importar cualquier modelo.
Este m√≥dulo configura la GPU para evitar fragmentaci√≥n de memoria.
"""

import os
import sys


def configure_gpu_for_training():
    """
    Configura GPU para entrenamiento secuencial de modelos.

    Caracter√≠sticas:
    - Memory Growth: Aloca memoria gradualmente en vez de reservar toda
    - CUDA Async Allocator: Reduce fragmentaci√≥n de memoria
    - Optimizado para entrenamientos m√∫ltiples sin reiniciar proceso

    Returns:
        bool: True si GPU configurada exitosamente, False si no hay GPU o fall√≥
    """

    print("=" * 70)
    print("üîß [GPU CONFIG] Inicializando configuraci√≥n GPU...")
    print("=" * 70)

    # ======================================================================
    # PASO 1: Configurar variables de ambiente (ANTES de import tensorflow)
    # ======================================================================

    # Memory Growth: Permite que TensorFlow aloque memoria gradualmente
    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
    print("   ‚úì TF_FORCE_GPU_ALLOW_GROWTH = true")

    # CUDA Async Allocator: Reduce fragmentaci√≥n de memoria
    # Este es el nuevo allocator de TensorFlow que maneja mejor la fragmentaci√≥n
    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'
    print("   ‚úì TF_GPU_ALLOCATOR = cuda_malloc_async")

    # Suprimir mensajes de info de TensorFlow (opcional)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0=all, 1=info, 2=warning, 3=error

    # ======================================================================
    # PASO 2: Importar y configurar TensorFlow program√°ticamente
    # ======================================================================

    try:
        import tensorflow as tf

        # Verificar GPUs disponibles
        gpus = tf.config.list_physical_devices('GPU')

        if not gpus:
            print("   ‚ö†Ô∏è No se detectaron GPUs f√≠sicas")
            print("   ‚ÑπÔ∏è Los modelos se entrenar√°n en CPU")
            print("=" * 70)
            return False

        # Configurar cada GPU detectada
        for i, gpu in enumerate(gpus):
            try:
                # CR√çTICO: Habilitar memory growth
                # Esto DEBE hacerse antes de cualquier operaci√≥n GPU
                tf.config.experimental.set_memory_growth(gpu, True)
                print(f"   ‚úì Memory Growth habilitado en GPU {i}: {gpu.name}")

            except RuntimeError as e:
                # Si falla, probablemente GPU ya fue inicializada
                print(f"   ‚ùå Error configurando GPU {i}: {e}")
                print(f"   ‚ö†Ô∏è GPU ya inicializada, configuraci√≥n puede no aplicarse")
                return False

        # Verificar GPUs l√≥gicas (despu√©s de configuraci√≥n)
        logical_gpus = tf.config.list_logical_devices('GPU')
        print(f"\n   üìä Resumen:")
        print(f"      - GPUs F√≠sicas: {len(gpus)}")
        print(f"      - GPUs L√≥gicas: {len(logical_gpus)}")

        # Mostrar informaci√≥n de cada GPU
        for i, gpu in enumerate(gpus):
            print(f"      - GPU {i}: {gpu.name}")

        print("\n   ‚úÖ Configuraci√≥n GPU completada exitosamente")
        print("   ‚ÑπÔ∏è Los modelos intentar√°n usar GPU primero, CPU como fallback")
        print("=" * 70)
        return True

    except ImportError:
        print("   ‚ùå ERROR: TensorFlow no instalado")
        print("=" * 70)
        return False

    except Exception as e:
        print(f"   ‚ùå ERROR inesperado configurando GPU: {e}")
        print("   ‚ö†Ô∏è Los modelos se entrenar√°n en CPU")
        print("=" * 70)
        return False


def print_gpu_memory_info():
    """
    Muestra informaci√≥n de memoria GPU (√∫til para debugging).

    Llamar DESPU√âS de configure_gpu_for_training()
    """
    try:
        import tensorflow as tf

        gpus = tf.config.list_physical_devices('GPU')
        if not gpus:
            print("[GPU INFO] No hay GPUs disponibles")
            return

        print("\n" + "=" * 70)
        print("üìä [GPU MEMORY INFO] Estado de memoria GPU")
        print("=" * 70)

        for i, gpu in enumerate(gpus):
            try:
                # Intentar obtener estad√≠sticas de memoria
                memory_info = tf.config.experimental.get_memory_info(f'GPU:{i}')

                current_mb = memory_info['current'] / (1024**2)
                peak_mb = memory_info['peak'] / (1024**2)

                print(f"\nGPU {i} ({gpu.name}):")
                print(f"   - Memoria Actual: {current_mb:.2f} MB")
                print(f"   - Memoria Pico:   {peak_mb:.2f} MB")

            except Exception as e:
                print(f"\nGPU {i}: No se pudo obtener info de memoria ({e})")

        print("=" * 70 + "\n")

    except Exception as e:
        print(f"[GPU INFO] Error: {e}")


# ======================================================================
# INICIALIZACI√ìN AUTOM√ÅTICA (cuando se importa este m√≥dulo)
# ======================================================================

if __name__ != "__main__":
    # Solo configurar si NO estamos ejecutando este archivo directamente
    # (es decir, si lo est√°n importando)
    _gpu_configured = configure_gpu_for_training()
else:
    # Si ejecutan este archivo directamente, mostrar info
    print(__doc__)
    _gpu_configured = configure_gpu_for_training()
    if _gpu_configured:
        print_gpu_memory_info()
