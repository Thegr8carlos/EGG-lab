# üìã PLAN DE MEJORAS - Sistema EEG Lab

**Fecha**: 2025-11-09
**Problemas identificados**: 4 √°reas cr√≠ticas

---

## üéØ PROBLEMA 1: Error en Pipeline (ICA + Wavelets)

### Diagn√≥stico
- **Error**: `FileNotFoundError: 'temp_step_1_input.npy'`
- **Causa ra√≠z**: El pipeline crea archivos intermedios pero no los enlaza correctamente entre pasos
- **Ubicaci√≥n**: `backend/classes/Experiment.py` l√≠neas 850-900
- **Impacto**: El pipeline no funciona cuando se combinan filtros + transformadas

### Soluci√≥n: 3 pasos incrementales

#### PASO 1.1: Arreglar flujo de archivos intermedios en pipeline
**Objetivo**: Asegurar que cada paso del pipeline encuentre el archivo de entrada correcto

**Cambios**:
- En `Experiment.py`, m√©todo `apply_model_pipeline()`:
  - Despu√©s de aplicar cada filtro, copiar expl√≠citamente el output como input del siguiente paso
  - Renombrar archivos intermedios con patr√≥n consistente
  - Agregar validaci√≥n de existencia de archivos antes de cada paso

**Testing esperado**:
```bash
# Probar aplicar ICA solo ‚Üí Debe funcionar
# Ver logs: ‚úÖ ICA aplicado correctamente
# Verificar que se crea el archivo en intermediates/
```

**Qu√© soluciona**: Previene el error de archivo no encontrado
**C√≥mo lo hace**: Crea enlaces expl√≠citos entre outputs/inputs de pasos

---

#### PASO 1.2: Mejorar logging y manejo de errores del pipeline
**Objetivo**: Detectar y reportar claramente cu√°ndo falla un paso

**Cambios**:
- Agregar try-catch espec√≠fico por cada paso del pipeline
- Logging detallado de:
  - Archivo de entrada de cada paso
  - Archivo de salida generado
  - Si el archivo existe despu√©s de aplicar
- Si un paso falla, retornar resultado parcial en lugar de crash completo

**Testing esperado**:
```bash
# Aplicar ICA + Wavelets
# Ver logs detallados:
# üìç Fase 1: Aplicando ICA
#   ‚Üí Entrada: temp_step_0_input.npy ‚úÖ
#   ‚Üí Salida: temp_step_0_output/... ‚úÖ
# üìç Fase 2: Aplicando Wavelets
#   ‚Üí Entrada: temp_step_1_input.npy ‚úÖ
#   ‚Üí Salida: temp_output/... ‚úÖ
```

**Qu√© soluciona**: Visibilidad completa de qu√© paso falla
**C√≥mo lo hace**: Logging granular + manejo de errores robusto

---

#### PASO 1.3: Validar pipeline completo con cache
**Objetivo**: Asegurar que el sistema de cache no interfiere con el pipeline

**Cambios**:
- Verificar que el hash del pipeline incluye TODOS los pasos (filtros + transforms)
- Si el cache es inv√°lido, forzar recalcular pipeline completo
- Agregar flag `force_recalculate` para testing

**Testing esperado**:
```bash
# Aplicar ICA + Wavelets por primera vez ‚Üí Calcular todo
# Aplicar ICA + Wavelets segunda vez ‚Üí Usar cache ‚ö°
# Cambiar par√°metro de ICA ‚Üí Invalidar cache, recalcular
```

**Qu√© soluciona**: Asegura que el pipeline siempre se aplica completamente
**C√≥mo lo hace**: Sistema de cache con invalidaci√≥n correcta

---

## üéØ PROBLEMA 2: Reorganizar P√°gina Modelado P300

### Diagn√≥stico
- **Situaci√≥n actual**: Step indicator est√° arriba, controles de navegaci√≥n/clase est√°n en `create_navigation_controls()`
- **Objetivo**: Mover controles de navegaci√≥n de canales y botones de clase al slide de "Metadata"
- **Ubicaci√≥n**: `app/pages/modelado_p300.py` l√≠neas 86-500

### Soluci√≥n: 2 pasos incrementales

#### PASO 2.1: Crear componente separado para controles de metadata
**Objetivo**: Separar controles de navegaci√≥n/clase en un componente reutilizable

**Cambios**:
- Crear funci√≥n `create_metadata_controls(meta)` que retorna:
  - Informaci√≥n del dataset (nombre, clases, canales, frecuencia)
  - Chips de clases (clickeables para seleccionar)
  - Navegaci√≥n de canales (‚Üê Anteriores | Siguientes ‚Üí)
- Los chips de clase seleccionada se agrandan (transform: scale(1.1))

**Testing esperado**:
```bash
# Ejecutar app, ir a /p300
# Ver que los controles aparecen en el lugar correcto
# Clic en chip de clase ‚Üí Se agranda y cambia visualizaci√≥n
# Navegaci√≥n de canales funciona igual que antes
```

**Qu√© soluciona**: Separa l√≥gica de metadata en componente reutilizable
**C√≥mo lo hace**: Extrae UI a funci√≥n independiente
**No rompe**: Los callbacks existentes siguen funcionando (mismos IDs)

---

#### PASO 2.2: Integrar controles en slide de metadata
**Objetivo**: Colocar el componente en el slide correcto

**Cambios**:
- Modificar layout de slide "Metadata" para incluir `create_metadata_controls()`
- Remover controles duplicados del layout principal
- Mantener todos los IDs exactamente iguales para no romper callbacks

**Testing esperado**:
```bash
# Ejecutar app, ir a /p300
# Cambiar entre slides ‚Üí Ver controles en slide "Metadata"
# Todos los callbacks funcionan: filtro de clase, navegaci√≥n, etc.
```

**Qu√© soluciona**: UI m√°s organizada y l√≥gica
**C√≥mo lo hace**: Mueve componentes sin cambiar IDs
**No rompe**: Callbacks existentes funcionan sin modificaci√≥n

---

## üéØ PROBLEMA 3: Visualizaci√≥n de Wavelets en Filtros

### Diagn√≥stico
- **S√≠ntoma**: En filtros.py, la transformada wavelet se ve como una l√≠nea plana
- **Causa probable**: Los datos de wavelet son 3D (ventanas) pero el plot espera 2D
- **Ubicaci√≥n**: `app/pages/filtros.py`, clientside callback de render

### Soluci√≥n: 2 pasos incrementales

#### PASO 3.1: Detectar datos 3D en callback clientside
**Objetivo**: Identificar cu√°ndo la transformada es 3D y ajustar visualizaci√≥n

**Cambios**:
- En clientside callback de `filtros.py`:
  - Detectar si `filteredData.matrix` es 3D (tiene .ndim == 3 o shape[0] es array)
  - Si es 3D, convertir a 2D para visualizaci√≥n:
    - Opci√≥n A: Mostrar solo la primera ventana
    - Opci√≥n B: Promediar todas las ventanas
    - Opci√≥n C: Aplanar (concatenar ventanas)

**Testing esperado**:
```bash
# Aplicar solo ICA ‚Üí Se ve bien (2D)
# Aplicar solo Wavelets ‚Üí Se ve bien (3D ‚Üí 2D convertido)
# Aplicar ICA + Wavelets ‚Üí Se ve bien
```

**Qu√© soluciona**: Visualizaci√≥n correcta de datos venteados
**C√≥mo lo hace**: Detecci√≥n de dimensionalidad + conversi√≥n

---

#### PASO 3.2: Agregar indicador visual de datos 3D
**Objetivo**: Informar al usuario que est√° viendo datos venteados

**Cambios**:
- Si los datos son 3D, agregar anotaci√≥n en el plot:
  - "Datos venteados (mostrando ventana 1 de N)"
  - Posici√≥n: esquina superior derecha
- Color diferente para datos 3D vs 2D

**Testing esperado**:
```bash
# Aplicar Wavelets ‚Üí Ver anotaci√≥n "Datos venteados (ventana 1 de 100)"
# Aplicar ICA ‚Üí No ver anotaci√≥n (datos 2D normales)
```

**Qu√© soluciona**: Claridad sobre qu√© tipo de datos se visualizan
**C√≥mo lo hace**: Anotaci√≥n condicional en plot

---

## üéØ PROBLEMA 4: Load Dataset Gen√©rico

### Diagn√≥stico
- **Situaci√≥n actual**: `Dataset.load_dataset()` est√° hardcodeado para Inner Speech
  - Usa IDs de eventos espec√≠ficos (31=arriba, 32=abajo, etc.)
  - Funci√≥n `_inner_speech_cues()` busca eventos espec√≠ficos (15, 22, 16)
- **Objetivo**: Hacer que funcione con cualquier dataset BDF/EDF

### Soluci√≥n: 4 pasos incrementales

#### PASO 4.1: Crear sistema de detecci√≥n de formato
**Objetivo**: Detectar autom√°ticamente si el dataset usa el formato Inner Speech o es gen√©rico

**Cambios**:
- Crear funci√≥n `_detect_dataset_format(events)`:
  - Busca event IDs espec√≠ficos de Inner Speech (15, 22, 31-34)
  - Si encuentra ‚Üí retorna "inner_speech"
  - Si no ‚Üí retorna "generic"
- Agregar par√°metro `format="auto"` a `load_dataset()`

**Testing esperado**:
```bash
# Cargar dataset Inner Speech ‚Üí Detecta "inner_speech" ‚úÖ
# Cargar otro dataset BDF ‚Üí Detecta "generic" ‚úÖ
```

**Qu√© soluciona**: Identifica autom√°ticamente el tipo de dataset
**C√≥mo lo hace**: Inspecci√≥n de event IDs

---

#### PASO 4.2: Implementar extracci√≥n gen√©rica de eventos
**Objetivo**: Extraer eventos sin asumir IDs espec√≠ficos

**Cambios**:
- Crear funci√≥n `_generic_event_extraction(raw, events)`:
  - Lista TODOS los event IDs √∫nicos
  - Asigna labels gen√©ricos: "Evento_1", "Evento_2", etc.
  - Permite al usuario mapear IDs ‚Üí nombres despu√©s
- Guardar mapping en `dataset_metadata.json`:
  ```json
  {
    "event_id_mapping": {
      "31": "arriba",  // Inner Speech
      "1": "Evento_1"  // Gen√©rico
    }
  }
  ```

**Testing esperado**:
```bash
# Cargar dataset gen√©rico
# Ver en metadata: event_id_mapping con todos los IDs encontrados
# Poder editar mapping manualmente si es necesario
```

**Qu√© soluciona**: Extrae eventos de cualquier dataset
**C√≥mo lo hace**: Usa todos los IDs √∫nicos encontrados

---

#### PASO 4.3: Ajustar generaci√≥n de archivos auxiliares
**Objetivo**: Generar Events, Labels, Raw para cualquier dataset

**Cambios**:
- Modificar `load_dataset()`:
  - Si format="inner_speech" ‚Üí Usa l√≥gica actual
  - Si format="generic":
    - Extrae epochs para cada event ID √∫nico
    - Crea carpeta `Events/` con archivos por ID
    - Genera `labels.npy` con IDs en lugar de nombres
    - Permite configurar par√°metros de epoching (tmin, tmax)

**Testing esperado**:
```bash
# Cargar dataset gen√©rico con 5 event IDs
# Ver carpeta Events/ con 5 subcarpetas
# Ver labels.npy con valores correspondientes
# Metadata JSON generado correctamente
```

**Qu√© soluciona**: Genera estructura completa para datasets custom
**C√≥mo lo hace**: Extracci√≥n gen√©rica de epochs + metad

ata

---

#### PASO 4.4: Crear UI para configurar datasets custom
**Objetivo**: Permitir al usuario configurar par√°metros de carga

**Cambios**:
- En p√°gina `cargar_datos.py`, agregar modal/formulario:
  - Par√°metros de epoching (tmin, tmax, baseline)
  - Mapping manual de event IDs ‚Üí nombres de clase
  - Selecci√≥n de canales a incluir
- Guardar configuraci√≥n en JSON para reutilizar

**Testing esperado**:
```bash
# Subir dataset custom
# Abrir modal de configuraci√≥n
# Ajustar tmin=-0.2, tmax=1.0
# Mapear ID 1 ‚Üí "Clase_A", ID 2 ‚Üí "Clase_B"
# Generar dataset ‚Üí Funciona con configuraci√≥n custom
```

**Qu√© soluciona**: Flexibilidad total para datasets custom
**C√≥mo lo hace**: UI interactiva + configuraci√≥n persistente

---

## üìä ORDEN DE EJECUCI√ìN RECOMENDADO

### Semana 1: Pipeline + P300
1. PASO 1.1 - Arreglar flujo archivos pipeline
2. PASO 1.2 - Logging y errores pipeline
3. PASO 1.3 - Validar cache pipeline
4. Probar ICA + Wavelets ‚Üí Debe funcionar ‚úÖ

### Semana 2: UI + Visualizaci√≥n
5. PASO 2.1 - Componente controles metadata
6. PASO 2.2 - Integrar en slide
7. PASO 3.1 - Detectar datos 3D
8. PASO 3.2 - Indicador visual 3D
9. Probar todas las transformadas ‚Üí Visualizan bien ‚úÖ

### Semana 3: Load Dataset Gen√©rico
10. PASO 4.1 - Detecci√≥n de formato
11. PASO 4.2 - Extracci√≥n gen√©rica
12. PASO 4.3 - Generaci√≥n archivos
13. PASO 4.4 - UI configuraci√≥n
14. Probar con dataset custom ‚Üí Funciona ‚úÖ

---

## ‚úÖ CRITERIOS DE √âXITO POR PROBLEMA

### Problema 1 (Pipeline)
- ‚úÖ ICA + Wavelets funciona sin errores
- ‚úÖ Logs muestran cada paso claramente
- ‚úÖ Cache funciona correctamente

### Problema 2 (P300 UI)
- ‚úÖ Controles en slide correcto
- ‚úÖ Callbacks funcionan sin cambios
- ‚úÖ Chips de clase se agrandan al seleccionar

### Problema 3 (Visualizaci√≥n)
- ‚úÖ Wavelets se visualizan correctamente
- ‚úÖ Datos 3D convierten a 2D
- ‚úÖ Indicador visual claro

### Problema 4 (Load Custom)
- ‚úÖ Detecta formato autom√°ticamente
- ‚úÖ Extrae eventos de cualquier dataset
- ‚úÖ Genera estructura completa
- ‚úÖ UI permite configurar par√°metros

---

## üö® NO ROMPER (Validaciones)

Despu√©s de cada paso, verificar:
- ‚úÖ Pipeline existente sigue funcionando
- ‚úÖ Historial de filtros/transforms se guarda
- ‚úÖ Callbacks no cambian de comportamiento
- ‚úÖ Stores mantienen mismos IDs
- ‚úÖ Cache sigue siendo v√°lido

---

**Siguiente acci√≥n**: Empezar con PASO 1.1 - Arreglar flujo archivos pipeline
